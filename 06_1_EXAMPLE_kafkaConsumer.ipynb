{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000018C1DBAA7C0>\n"
     ]
    },
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x18c1dbaa7c0>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.4.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Kafka</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import SparkSession\n",
    "import ConnectionConfig as cc\n",
    "cc.setupEnvironment()\n",
    "spark = cc.startLocalCluster(\"Kafka\")\n",
    "spark.getActiveSession()\n",
    "wordcount_delta_path = \"./spark-warehouse/kafkaWordCount\"\n",
    "#spark.sparkContext.setLogLevel(\"DEBUG\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T13:41:24.803755Z",
     "end_time": "2023-10-04T13:41:45.342102Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Kafka\n",
    "In this cell reading from a Kafka topic is initiated.\n",
    "The enabled code gets the data from a docker container running Kafka.\n",
    "_The commented code gets the data from a Confluent Cloud Kafka instance. At this point the code with Confluent is not working. With debugging on it seems that the connection is made with the server but the server disconnects imeadiately after the connection is made. The error message is not very helpful._"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df = (\n",
    "     spark.readStream\n",
    "     .format(\"kafka\")\n",
    "     .option(\"kafka.bootstrap.servers\", \"127.0.0.1:29092\")\n",
    "     .option(\"subscribe\", \"demo\")\n",
    "     #.option(\"startingOffsets\", \"earliest\")\n",
    "     .load()\n",
    " )\n",
    "# spark.sparkContext.setLogLevel(\"DEBUG\")\n",
    "# df =  (spark.readStream\n",
    "#      .format(\"kafka\")\n",
    "#      .option(\"kafka.bootstrap.servers\", \"pkc-6ojv2.us-west4.gcp.confluent.cloud:9092\")\n",
    "#      .option(\"security.protocol\", \"SASL_SSL\")\n",
    "#      .option(\"sasl.jaas.config\", \"org.apache.kafka.common.security.plain.PlainLoginModule required username='PUT API USERNAME HIER password='PUT API KEY PASSWORD HERE';\")\n",
    "#      .option(\"sasl.mechanism\", \"PLAIN\")\n",
    "#      #.option(\"kafka.group.id\", \"test\")\n",
    "#      .option(\"client.dns.lookup\",\"use_all_dns_ips\")\n",
    "#      .option(\"ssl.endpoint.identification.algorithm\", \"https\")\n",
    "#      .option(\"session.timeout.ms\", \"45000\")\n",
    "#      .option(\"acks\", \"all\")\n",
    "#      .option(\"schema.registry.url\", \"https://psrc-30dr2.us-central1.gcp.confluent.cloud\")\n",
    "#      .option(\"basci.auth.credentials.source\", \"USER_INFO\")\n",
    "#      .option(\"basic.auth.user.info\", \"PPZSKZFILMHO5GFA:sn0rEL43vd2Uz8EUDwWNST511BM1UoggZW0m1PeiZg0VuEilOPYIBKFUwIe3jNgd\")\n",
    "#      .option(\"subscribe\", \"demo\")\n",
    "#      .option(\"kafka.session.timeout.ms\", \"10000\")\n",
    "#      .option(\"startingOffsets\", \"earliest\")\n",
    "#         .load())\n",
    "\n",
    "#df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T13:42:03.471021Z",
     "end_time": "2023-10-04T13:42:05.837311Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Converting the data to a dataframe\n",
    "json_tuple is an sql function that converts a json string to dataframe columns. The columns are named c0, c1, c2.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- c0: string (nullable = true)\n",
      " |-- c1: string (nullable = true)\n",
      " |-- c2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "#df.writeStream.format(\"console\").outputMode(\"update\").option(\"checkpointLocation\",\".\\checkpoints\").start(\"kafkaTest\")\n",
    "inter = df.selectExpr(\"json_tuple(CAST(value as STRING), 'timestamp', 'key', 'value')\")\n",
    "inter.printSchema()\n",
    "#stream = inter.writeStream.format(\"console\").outputMode(\"update\").option(\"checkpointLocation\",\".\\checkpoints\").start(\"kafkaTest\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T13:42:11.485498Z",
     "end_time": "2023-10-04T13:42:11.732490Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A string is converted to a timestamp and column names are changed to eventtime, key and value."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- eventtime: timestamp (nullable = true)\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordCountEvents= inter.selectExpr(\"cast(c0 as timestamp) as eventtime\" , \"c1 as key\", \"INT(c2) as value\").withWatermark(\"eventtime\",'10 seconds')\n",
    "wordCountEvents.printSchema()\n",
    "stream = wordCountEvents.writeStream.format(\"console\").outputMode(\"update\").option(\"checkpointLocation\",\".\\checkpoints1\").start(\"kafkaTest\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T13:42:47.687002Z",
     "end_time": "2023-10-04T13:42:49.204770Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Wordcount aggregation\n",
    "The data is aggregated by a 10 second window and the key. The result is written to a delta table.\n",
    "The results are written to a delatable in append mode. This means that the data is added to the table after the window is closed after the watermark is passed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- key: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groupedEvents =wordCountEvents.groupBy(F.window(\"eventtime\", \"10 seconds\", \"10 seconds\"), \"key\").sum(\"value\").withColumnRenamed(\"sum(value)\", \"count\")\n",
    "groupedEvents.printSchema()\n",
    "stream = groupedEvents.writeStream.format(\"delta\").outputMode(\"append\").option(\"checkpointLocation\",\".\\checkpoints\").option(\"path\", wordcount_delta_path).start()\n",
    "#stream = groupedEvents.writeStream.format(\"console\").outputMode(\"update\").option(\"checkpointLocation\",\".\\checkpoints2\").start()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T13:43:13.370138Z",
     "end_time": "2023-10-04T13:43:14.765620Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "stream.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T11:48:20.746797Z",
     "end_time": "2023-10-04T11:48:20.772827Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T08:57:10.910147Z",
     "end_time": "2023-10-04T08:57:11.691142Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
