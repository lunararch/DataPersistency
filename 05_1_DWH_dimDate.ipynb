{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T09:34:00.228763100Z",
     "start_time": "2024-09-23T09:33:58.899526900Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "import ConnectionConfig as cc\n",
    "cc.setupEnvironment()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Start the cluster\n",
    "Look at the getActiveSession() method in the ConnectionConfig.py file. It will return the active session. It will also add the delta package to the session and add extra jars to the session. The jars are needed to connect to the SQL Server database."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T09:35:13.139154300Z",
     "start_time": "2024-09-23T09:34:37.276478900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x17808c289d0>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.140.98.64:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.5.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>DIM_DATE</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = cc.startLocalCluster(\"DIM_DATE\",4)\n",
    "spark.getActiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Date dimension from scratch\n",
    "\n",
    "In this example we will build a date dimension from scratch.\n",
    "\n",
    "## Step 1: Generate rows for a sequence of dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T09:43:53.772712100Z",
     "start_time": "2024-09-10T09:43:46.304713900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|calendarDate|dateSK|\n",
      "+------------+------+\n",
      "|  2009-01-01|     0|\n",
      "|  2009-01-02|     1|\n",
      "|  2009-01-03|     2|\n",
      "|  2009-01-04|     3|\n",
      "|  2009-01-05|     4|\n",
      "|  2009-01-06|     5|\n",
      "|  2009-01-07|     6|\n",
      "|  2009-01-08|     7|\n",
      "|  2009-01-09|     8|\n",
      "|  2009-01-10|     9|\n",
      "|  2009-01-11|    10|\n",
      "|  2009-01-12|    11|\n",
      "|  2009-01-13|    12|\n",
      "|  2009-01-14|    13|\n",
      "|  2009-01-15|    14|\n",
      "|  2009-01-16|    15|\n",
      "|  2009-01-17|    16|\n",
      "|  2009-01-18|    17|\n",
      "|  2009-01-19|    18|\n",
      "|  2009-01-20|    19|\n",
      "+------------+------+\n"
     ]
    }
   ],
   "source": [
    "#extract\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "beginDate = '2009-01-01'\n",
    "endDate = '2023-12-31'\n",
    "\n",
    "df_SQL = spark.sql(f\"select explode(sequence(to_date('{beginDate}'), to_date('{endDate}'), interval 1 day)) as calendarDate, monotonically_increasing_id() as dateSK \")\n",
    "\n",
    "\n",
    "df_SQL.createOrReplaceTempView('neededDates' )\n",
    "\n",
    "spark.sql(\"select * from neededDates\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example a dataframe df_SQL is created based on the result of a select statement:\n",
    "* ```spark.sql``` is used to create date rows with sql-like language. You can find all possible SQL functions [here](https://spark.apache.org/docs/latest/api/sql/)\n",
    "*  [```sequence```](https://spark.apache.org/docs/latest/api/sql/#sequence) creates a list of dates between the begin and end date. The interval is 1 day., [```explode```](https://spark.apache.org/docs/latest/api/sql/#explode) generates a row for each item in the array.\n",
    "* [```monotonically_increasing_id```](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.monotonically_increasing_id.html#pyspark.sql.functions.monotonically_increasing_id) is used to generate a unique id in a clustered environment\n",
    "\n",
    "The dataframe is made available to use as a table called \"neededDates\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create all typical dimension fields\n",
    "Because we want to represent the date in different ways (weekday, month...), we have to do several tranformations. You can see an extract of our go\n",
    "```\n",
    "+------+--------+------------+------------+-------------+-----------+-----------+---------+--------------------+---------+----------+----------------+---------+-------------+-------------+\n",
    "|dateSK| dateInt|CalendarDate|CalendarYear|CalendarMonth|MonthOfYear|CalendarDay|DayOfWeek|DayOfWeekStartMonday|IsWeekDay|DayOfMonth|IsLastDayOfMonth|DayOfYear|WeekOfYearIso|QuarterOfYear|\n",
    "+------+--------+------------+------------+-------------+-----------+-----------+---------+--------------------+---------+----------+----------------+---------+-------------+-------------+\n",
    "|     0|20090101|  2009-01-01|        2009|      January|          1|   Thursday|        5|                   4|        Y|         1|               N|        1|            1|            1|\n",
    "|     1|20090102|  2009-01-02|        2009|      January|          1|     Friday|        6|                   5|        Y|         2|               N|        2|            1|            1|\n",
    "|     2|20090103|  2009-01-03|        2009|      January|          1|   Saturday|        7|                   6|        N|         3|               N|        3|            1|            1|\n",
    "|     3|20090104|  2009-01-04|        2009|      January|          1|     Sunday|        1|                   7|        N|         4|               N|        4|            1|            1|\n",
    "```\n",
    "\n",
    "### Method a: Use spark.sql to perform all the transformations with the help of a sql-query.\n",
    "For many, creating an SQL-select statement is the most easy way to perform the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T09:43:55.426099Z",
     "start_time": "2024-09-10T09:43:53.749254500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------+------------+-------------+-----------+-----------+---------+--------------------+---------+----------+----------------+---------+-------------+-------------+\n",
      "|dateSK| dateInt|CalendarDate|CalendarYear|CalendarMonth|MonthOfYear|CalendarDay|DayOfWeek|DayOfWeekStartMonday|IsWeekDay|DayOfMonth|IsLastDayOfMonth|DayOfYear|WeekOfYearIso|QuarterOfYear|\n",
      "+------+--------+------------+------------+-------------+-----------+-----------+---------+--------------------+---------+----------+----------------+---------+-------------+-------------+\n",
      "|     0|20090101|  2009-01-01|        2009|      January|          1|   Thursday|        5|                   4|        Y|         1|               N|        1|            1|            1|\n",
      "|     1|20090102|  2009-01-02|        2009|      January|          1|     Friday|        6|                   5|        Y|         2|               N|        2|            1|            1|\n",
      "|     2|20090103|  2009-01-03|        2009|      January|          1|   Saturday|        7|                   6|        N|         3|               N|        3|            1|            1|\n",
      "|     3|20090104|  2009-01-04|        2009|      January|          1|     Sunday|        1|                   7|        N|         4|               N|        4|            1|            1|\n",
      "|     4|20090105|  2009-01-05|        2009|      January|          1|     Monday|        2|                   1|        Y|         5|               N|        5|            2|            1|\n",
      "|     5|20090106|  2009-01-06|        2009|      January|          1|    Tuesday|        3|                   2|        Y|         6|               N|        6|            2|            1|\n",
      "|     6|20090107|  2009-01-07|        2009|      January|          1|  Wednesday|        4|                   3|        Y|         7|               N|        7|            2|            1|\n",
      "|     7|20090108|  2009-01-08|        2009|      January|          1|   Thursday|        5|                   4|        Y|         8|               N|        8|            2|            1|\n",
      "|     8|20090109|  2009-01-09|        2009|      January|          1|     Friday|        6|                   5|        Y|         9|               N|        9|            2|            1|\n",
      "|     9|20090110|  2009-01-10|        2009|      January|          1|   Saturday|        7|                   6|        N|        10|               N|       10|            2|            1|\n",
      "|    10|20090111|  2009-01-11|        2009|      January|          1|     Sunday|        1|                   7|        N|        11|               N|       11|            2|            1|\n",
      "|    11|20090112|  2009-01-12|        2009|      January|          1|     Monday|        2|                   1|        Y|        12|               N|       12|            3|            1|\n",
      "|    12|20090113|  2009-01-13|        2009|      January|          1|    Tuesday|        3|                   2|        Y|        13|               N|       13|            3|            1|\n",
      "|    13|20090114|  2009-01-14|        2009|      January|          1|  Wednesday|        4|                   3|        Y|        14|               N|       14|            3|            1|\n",
      "|    14|20090115|  2009-01-15|        2009|      January|          1|   Thursday|        5|                   4|        Y|        15|               N|       15|            3|            1|\n",
      "|    15|20090116|  2009-01-16|        2009|      January|          1|     Friday|        6|                   5|        Y|        16|               N|       16|            3|            1|\n",
      "|    16|20090117|  2009-01-17|        2009|      January|          1|   Saturday|        7|                   6|        N|        17|               N|       17|            3|            1|\n",
      "|    17|20090118|  2009-01-18|        2009|      January|          1|     Sunday|        1|                   7|        N|        18|               N|       18|            3|            1|\n",
      "|    18|20090119|  2009-01-19|        2009|      January|          1|     Monday|        2|                   1|        Y|        19|               N|       19|            4|            1|\n",
      "|    19|20090120|  2009-01-20|        2009|      January|          1|    Tuesday|        3|                   2|        Y|        20|               N|       20|            4|            1|\n",
      "+------+--------+------------+------------+-------------+-----------+-----------+---------+--------------------+---------+----------+----------------+---------+-------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "dimDate = spark.sql(\"select dateSK, \\\n",
    "  year(calendarDate) * 10000 + month(calendarDate) * 100 + day(calendarDate) as dateInt, \\\n",
    "  CalendarDate, \\\n",
    "  year(calendarDate) AS CalendarYear, \\\n",
    "  date_format(calendarDate, 'MMMM') as CalendarMonth, \\\n",
    "  month(calendarDate) as MonthOfYear, \\\n",
    "  date_format(calendarDate, 'EEEE') as CalendarDay, \\\n",
    "  dayofweek(calendarDate) AS DayOfWeek, \\\n",
    "  weekday(calendarDate) + 1 as DayOfWeekStartMonday, \\\n",
    "  case \\\n",
    "    when weekday(calendarDate) < 5 then 'Y' \\\n",
    "    else 'N' \\\n",
    "  end as IsWeekDay, \\\n",
    "  dayofmonth(calendarDate) as DayOfMonth, \\\n",
    "  case \\\n",
    "    when calendarDate = last_day(calendarDate) then 'Y' \\\n",
    "    else 'N' \\\n",
    "  end as IsLastDayOfMonth, \\\n",
    "  dayofyear(calendarDate) as DayOfYear, \\\n",
    "  weekofyear(calendarDate) as WeekOfYearIso, \\\n",
    "  quarter(calendarDate) as QuarterOfYear \\\n",
    "from  \\\n",
    "  neededDates \\\n",
    "order by \\\n",
    "  calendarDate\")\n",
    "\n",
    "dimDate.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ```spark.sql``` is used to create the select query and returns the desired DataFrame. Remember to look-up the possible functions [here](https://spark.apache.org/docs/latest/api/sql/).\n",
    "* ```dimDate.show()``` s used to show the records in a DataFrame. Use it during development, but disable when not needed anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Method b: Use the dataframe API\n",
    "\n",
    "This method does not use the sql-like language. You can achieve the same with this method and you get better code completion. See [DataFrame API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html)\n",
    "As an example two columns where added with   ```withColumn``` .\n",
    "```Ã¨xpr()``` is used to write a snippet of 'sql' code and parse it into a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T09:43:56.076849400Z",
     "start_time": "2024-09-10T09:43:55.434863300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+----+-------+--------------+\n",
      "|calendarDate|dateSK|year|  month|lasyDayOfMonth|\n",
      "+------------+------+----+-------+--------------+\n",
      "|  2009-01-01|     0|2009|January|             N|\n",
      "|  2009-01-02|     1|2009|January|             N|\n",
      "|  2009-01-03|     2|2009|January|             N|\n",
      "|  2009-01-04|     3|2009|January|             N|\n",
      "|  2009-01-05|     4|2009|January|             N|\n",
      "|  2009-01-06|     5|2009|January|             N|\n",
      "|  2009-01-07|     6|2009|January|             N|\n",
      "|  2009-01-08|     7|2009|January|             N|\n",
      "|  2009-01-09|     8|2009|January|             N|\n",
      "|  2009-01-10|     9|2009|January|             N|\n",
      "|  2009-01-11|    10|2009|January|             N|\n",
      "|  2009-01-12|    11|2009|January|             N|\n",
      "|  2009-01-13|    12|2009|January|             N|\n",
      "|  2009-01-14|    13|2009|January|             N|\n",
      "|  2009-01-15|    14|2009|January|             N|\n",
      "|  2009-01-16|    15|2009|January|             N|\n",
      "|  2009-01-17|    16|2009|January|             N|\n",
      "|  2009-01-18|    17|2009|January|             N|\n",
      "|  2009-01-19|    18|2009|January|             N|\n",
      "|  2009-01-20|    19|2009|January|             N|\n",
      "+------------+------+----+-------+--------------+\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql.functions import explode, expr, sequence,col, date_format\n",
    "df_SparkSQL = df_SQL \\\n",
    "    .withColumn(\"year\", date_format(\"calendarDate\",'yyyy')) \\\n",
    "    .withColumn(\"month\", date_format(\"calendarDate\",'MMMM')) \\\n",
    "    .withColumn(\"lasyDayOfMonth\" \\\n",
    "                ,expr(\"case when calendarDate = last_day(calendarDate) then 'Y' \\\n",
    "                else 'N' \\\n",
    "                end as IsLastDayOfMonth\"))\n",
    "df_SparkSQL.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ## TASK:\n",
    "> Complete the transformation in method b until the result matches the result of method a."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Writing the data to a delta-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T09:44:14.542094Z",
     "start_time": "2024-09-10T09:43:56.060408800Z"
    }
   },
   "outputs": [],
   "source": [
    "dimDate.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dimDate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T09:44:15.370291500Z",
     "start_time": "2024-09-10T09:44:14.547115700Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:44:15.406348200Z",
     "start_time": "2024-09-10T09:44:15.371818100Z"
    }
   }
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
