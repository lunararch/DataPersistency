{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-03T12:19:11.663113Z",
     "start_time": "2024-11-03T12:19:11.580389Z"
    }
   },
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "import ConnectionConfig as cc\n",
    "cc.setupEnvironment()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Start the cluster\n",
    "Look at the getActiveSession() method in the ConnectionConfig.py file. It will return the active session. It will also add the delta package to the session and add extra jars to the session. The jars are needed to connect to the SQL Server database."
   ],
   "id": "256b3db176b9e198"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T12:19:18.415875Z",
     "start_time": "2024-11-03T12:19:11.666122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = cc.startLocalCluster(\"DIM_DATE\",4)\n",
    "spark.getActiveSession()"
   ],
   "id": "a18c7d6323a9935d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x16828acd350>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.43:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DIM_DATE</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Creating Date dimension\n",
    "\n",
    "\n",
    "## Step 1: Generate rows for a sequence of dates"
   ],
   "id": "d0966edd5525ac64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T12:19:21.748094Z",
     "start_time": "2024-11-03T12:19:19.229788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# EXTRACT\n",
    "beginDate = '2019-01-01'\n",
    "endDate = '2024-12-31'\n",
    "\n",
    "df_SQL = spark.sql(f\"select explode(sequence(to_date('{beginDate}'), to_date('{endDate}'), interval 1 day)) as calendarDate, monotonically_increasing_id() as date_SK \")\n",
    "\n",
    "\n",
    "df_SQL.createOrReplaceTempView('neededDates')\n",
    "\n",
    "spark.sql(\"select * from neededDates\").show(n=20)"
   ],
   "id": "9c0fd36d2d50a97b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|calendarDate|date_SK|\n",
      "+------------+-------+\n",
      "|  2019-01-01|      0|\n",
      "|  2019-01-02|      1|\n",
      "|  2019-01-03|      2|\n",
      "|  2019-01-04|      3|\n",
      "|  2019-01-05|      4|\n",
      "|  2019-01-06|      5|\n",
      "|  2019-01-07|      6|\n",
      "|  2019-01-08|      7|\n",
      "|  2019-01-09|      8|\n",
      "|  2019-01-10|      9|\n",
      "|  2019-01-11|     10|\n",
      "|  2019-01-12|     11|\n",
      "|  2019-01-13|     12|\n",
      "|  2019-01-14|     13|\n",
      "|  2019-01-15|     14|\n",
      "|  2019-01-16|     15|\n",
      "|  2019-01-17|     16|\n",
      "|  2019-01-18|     17|\n",
      "|  2019-01-19|     18|\n",
      "|  2019-01-20|     19|\n",
      "+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [],
   "id": "92e8441d2561d54c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T12:19:22.490051Z",
     "start_time": "2024-11-03T12:19:21.777354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TRANSFORM\n",
    "dimDate = spark.sql(\"select date_SK, \\\n",
    "  year(calendarDate) * 10000 + month(calendarDate) * 100 + day(calendarDate) as dateInt, \\\n",
    "  CalendarDate, \\\n",
    "  year(calendarDate) AS CalendarYear, \\\n",
    "  date_format(calendarDate, 'MMMM') as CalendarMonth, \\\n",
    "  month(calendarDate) as MonthOfYear, \\\n",
    "  date_format(calendarDate, 'EEEE') as CalendarDay, \\\n",
    "  dayofweek(calendarDate) AS DayOfWeek, \\\n",
    "  weekday(calendarDate) + 1 as DayOfWeekStartMonday, \\\n",
    "  case \\\n",
    "    when weekday(calendarDate) < 5 then 'Y' \\\n",
    "    else 'N' \\\n",
    "  end as IsWeekDay, \\\n",
    "  dayofmonth(calendarDate) as DayOfMonth, \\\n",
    "  case \\\n",
    "    when calendarDate = last_day(calendarDate) then 'Y' \\\n",
    "    else 'N' \\\n",
    "  end as IsLastDayOfMonth, \\\n",
    "  dayofyear(calendarDate) as DayOfYear, \\\n",
    "  weekofyear(calendarDate) as WeekOfYearIso, \\\n",
    "  quarter(calendarDate) as QuarterOfYear \\\n",
    "from  \\\n",
    "  neededDates \\\n",
    "order by \\\n",
    "  calendarDate\")\n",
    "\n",
    "dimDate.show()"
   ],
   "id": "6e87bc798265e9fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------+------------+-------------+-----------+-----------+---------+--------------------+---------+----------+----------------+---------+-------------+-------------+\n",
      "|date_SK| dateInt|CalendarDate|CalendarYear|CalendarMonth|MonthOfYear|CalendarDay|DayOfWeek|DayOfWeekStartMonday|IsWeekDay|DayOfMonth|IsLastDayOfMonth|DayOfYear|WeekOfYearIso|QuarterOfYear|\n",
      "+-------+--------+------------+------------+-------------+-----------+-----------+---------+--------------------+---------+----------+----------------+---------+-------------+-------------+\n",
      "|      0|20190101|  2019-01-01|        2019|      January|          1|    Tuesday|        3|                   2|        Y|         1|               N|        1|            1|            1|\n",
      "|      1|20190102|  2019-01-02|        2019|      January|          1|  Wednesday|        4|                   3|        Y|         2|               N|        2|            1|            1|\n",
      "|      2|20190103|  2019-01-03|        2019|      January|          1|   Thursday|        5|                   4|        Y|         3|               N|        3|            1|            1|\n",
      "|      3|20190104|  2019-01-04|        2019|      January|          1|     Friday|        6|                   5|        Y|         4|               N|        4|            1|            1|\n",
      "|      4|20190105|  2019-01-05|        2019|      January|          1|   Saturday|        7|                   6|        N|         5|               N|        5|            1|            1|\n",
      "|      5|20190106|  2019-01-06|        2019|      January|          1|     Sunday|        1|                   7|        N|         6|               N|        6|            1|            1|\n",
      "|      6|20190107|  2019-01-07|        2019|      January|          1|     Monday|        2|                   1|        Y|         7|               N|        7|            2|            1|\n",
      "|      7|20190108|  2019-01-08|        2019|      January|          1|    Tuesday|        3|                   2|        Y|         8|               N|        8|            2|            1|\n",
      "|      8|20190109|  2019-01-09|        2019|      January|          1|  Wednesday|        4|                   3|        Y|         9|               N|        9|            2|            1|\n",
      "|      9|20190110|  2019-01-10|        2019|      January|          1|   Thursday|        5|                   4|        Y|        10|               N|       10|            2|            1|\n",
      "|     10|20190111|  2019-01-11|        2019|      January|          1|     Friday|        6|                   5|        Y|        11|               N|       11|            2|            1|\n",
      "|     11|20190112|  2019-01-12|        2019|      January|          1|   Saturday|        7|                   6|        N|        12|               N|       12|            2|            1|\n",
      "|     12|20190113|  2019-01-13|        2019|      January|          1|     Sunday|        1|                   7|        N|        13|               N|       13|            2|            1|\n",
      "|     13|20190114|  2019-01-14|        2019|      January|          1|     Monday|        2|                   1|        Y|        14|               N|       14|            3|            1|\n",
      "|     14|20190115|  2019-01-15|        2019|      January|          1|    Tuesday|        3|                   2|        Y|        15|               N|       15|            3|            1|\n",
      "|     15|20190116|  2019-01-16|        2019|      January|          1|  Wednesday|        4|                   3|        Y|        16|               N|       16|            3|            1|\n",
      "|     16|20190117|  2019-01-17|        2019|      January|          1|   Thursday|        5|                   4|        Y|        17|               N|       17|            3|            1|\n",
      "|     17|20190118|  2019-01-18|        2019|      January|          1|     Friday|        6|                   5|        Y|        18|               N|       18|            3|            1|\n",
      "|     18|20190119|  2019-01-19|        2019|      January|          1|   Saturday|        7|                   6|        N|        19|               N|       19|            3|            1|\n",
      "|     19|20190120|  2019-01-20|        2019|      January|          1|     Sunday|        1|                   7|        N|        20|               N|       20|            3|            1|\n",
      "+-------+--------+------------+------------+-------------+-----------+-----------+---------+--------------------+---------+----------+----------------+---------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T12:19:26.840248Z",
     "start_time": "2024-11-03T12:19:22.535982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# LOAD\n",
    "dimDate.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dim_date\")\n"
   ],
   "id": "30d7ee70280286c7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T12:19:27.481938Z",
     "start_time": "2024-11-03T12:19:26.848684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark.stop()"
   ],
   "id": "4e2031931378cdb1",
   "outputs": [],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
