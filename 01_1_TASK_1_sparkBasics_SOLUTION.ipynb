{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SparkBasicsTask\n",
    "> 1. Run the first cell to start the Spark Server\n",
    "> 2. Run the second cell to excecute the wordcount code\n",
    "> 3. Go to the SparkUI Url. How many job and stages are there and why?\n",
    "> _One job, two stages. Two stages are needed because of the shuffle operation in the reduceByKey transformation. The data with the same key should land on the same cluster and has to be transferred over the network._\n",
    "> 4. Copy/paste te wordcount code in a new cell.\n",
    "> 5. In the coppied cells you have to add lines of code after the reduceByKey to also save the counts of first letter of the word (this way we know how many times a word starting with \"a\" appears in the text. The wordCount and letterCount must be saved seperately. After completing run the cell with the new code.\n",
    "> _Tip: Python key values are represented by Tuples. To change the (word, count) tuple to (letter,count) you kan use a map function with (wordTuple[0][0],wordTuple[1]). The first [0] represents the first element of the tuple (the key). The second [0] is the first letter of the key string)\n",
    "> 6. Go to the SparkUI Url. How many jobs are created? Why? Inspect the DAG of the 2 last jobs.\n",
    "> _The engine creates a job for every \"action\" in the code. For every action it will create a new excecution plan and all tranformations will be processed twice._\n",
    "> 7. Go to the Storage tab. Notice the tab is empty.\n",
    "> 8. To avoid Spark from excecuting all the steps twice add wordCounts.cache() after the reduceByKey.\n",
    "> 9. Run the code again. How many jobs are created now? Why? Inspect the DAG of the 2 last jobs. What is the difference with the previous DAGS? Compare the jobs by opening the DAGs in two different windows.\n",
    "> 10. Go to the Storage tab. Notice the tab is not empty anymore. Inspect the storage tab.\n",
    "> _By enabling caching, the first job will store the intermediate result. The second job will reuse this result, which will prevent processing the same steps twice. With caching, you can choose between processing and memory. When enough memory is available you can greatly increase the performance of the jobs._\n",
    "> 11. Excecute sc.stop() to stop the Spark Server\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "#Make sure pyspark is installed as a package of your project.\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import ConnectionConfig as cc\n",
    "cc.setupEnvironment()\n",
    "conf = SparkConf().setAppName(\"firstJob\").setMaster(\"local[*]\").setIfMissing(\"spark.logLineage\", \"true\")\n",
    "sc =SparkContext.getOrCreate((conf))\n",
    "sc.uiWebUrl"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T11:16:44.061004Z",
     "start_time": "2024-09-24T11:16:40.962136Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://192.168.1.12:4040'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "\n",
    "lines = sc.textFile(\".//FileStore//tables//shakespeare.txt\")\n",
    "words = lines.flatMap(lambda line: line.split(\" \")).filter(lambda word: len(word) > 0)\n",
    "wordKv = words.map(lambda word: (word, 1))\n",
    "wordCounts = wordKv.reduceByKey(lambda a,b:a +b)\n",
    "wordCounts.saveAsTextFile(\"./output/words\" + datetime.datetime.now().strftime(\"%m%d%Y%H%M%S\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T11:16:56.923841Z",
     "start_time": "2024-09-24T11:16:52.709896Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "\n",
    "lines = sc.textFile(\"./FileStore/tables/shakespeare.txt\")\n",
    "words = lines.flatMap(lambda line: line.split(\" \")).filter(lambda word: len(word) > 0)\n",
    "wordKv = words.map(lambda word: (word, 1))\n",
    "wordCounts = wordKv.reduceByKey(lambda a,b:a +b)\n",
    "#print(wordCounts.first())\n",
    "#wordCounts.cache()\n",
    "letterCounts = wordCounts.map(lambda wordTuple: (wordTuple[0][0],wordTuple[1]) )\n",
    "letterCounts = letterCounts.reduceByKey(lambda a,b:a +b)\n",
    "wordCounts.saveAsTextFile(\"./output/words\" + datetime.datetime.now().strftime(\"%m%d%Y%H%M%S\"))\n",
    "letterCounts.saveAsTextFile(\"./output/letters\" + datetime.datetime.now().strftime(\"%m%d%Y%H%M%S\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T11:17:04.297084Z",
     "start_time": "2024-09-24T11:16:59.280357Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "sc.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T11:17:06.299843Z",
     "start_time": "2024-09-24T11:17:05.455013Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
