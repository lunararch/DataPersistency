{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:39:17.415865100Z",
     "start_time": "2024-09-10T09:38:52.856653700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x213ccc7e950>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://AKDGPORT11191.mshome.net:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.5.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>SQLExcercise</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "import ConnectionConfig as cc\n",
    "cc.setupEnvironment()\n",
    "spark = cc.startLocalCluster(\"SQLExcercise\")\n",
    "spark.getActiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Go to https://spark.apache.org/docs/latest/sql-getting-started.html and https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#Quickstart:-DataFrame to get some insights in coding Spark SQL. Always select 'Python' as the language.\n",
    "\n",
    "Use the Spark SQL Reference documentation to complete this excercise\n",
    "- To write dataframe operations: Python SparkSQL API: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html\n",
    "- To write pure SQL statements: Spark SQL API: https://spark.apache.org/docs/2.3.0/api/sql/index.html and https://spark.apache.org/docs/latest/sql-ref.html\n",
    "Helpfull site with examples: https://sparkbyexamples.com/pyspark/\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load employees.csv as a Spark Dataframe\n",
    "Tip: https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#Getting-Data-In/Out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#Extract \n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .load(\"./FileStore/tables/employees.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:39:23.903089700Z",
     "start_time": "2024-09-10T09:39:17.398249900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Display the schema of the DataFrame\n",
    "Tip: https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#Getting-Data-In/Out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:39:23.914858200Z",
     "start_time": "2024-09-10T09:39:23.904186700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a temperary view of the dataset with name tbl_employees\n",
    "https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#Getting-Data-In/Out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"tbl_employees\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:39:23.976198500Z",
     "start_time": "2024-09-10T09:39:23.915951400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Calculate the total number of employees in two ways:\n",
    "-   Via dataframe operations: Tip: https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#Grouping-Data\n",
    "-   With a sql statement op tbl_employees: use spark.sql()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|   10|\n",
      "+-----+\n"
     ]
    }
   ],
   "source": [
    "df.groupBy().count().show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:39:24.853720400Z",
     "start_time": "2024-09-10T09:39:23.978350900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|count(employee_id)|\n",
      "+------------------+\n",
      "|                10|\n",
      "+------------------+\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(employee_id) from tbl_employees\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:39:25.581752900Z",
     "start_time": "2024-09-10T09:39:24.856920200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Find the average salary of all employees in two ways:\n",
    "-   Via the dataframe operation 'select'\n",
    "-   With a sql statement ont tbl_employees"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(salary)|\n",
      "+-----------+\n",
      "|     4820.0|\n",
      "+-----------+\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.select(avg(\"salary\")).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:39:26.706670500Z",
     "start_time": "2024-09-10T09:39:25.387956400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(salary)|\n",
      "+-----------+\n",
      "|     4820.0|\n",
      "+-----------+\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select avg(salary) from tbl_employees\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:39:27.083371200Z",
     "start_time": "2024-09-10T09:39:26.707790200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get the explain plan of the sql statement\n",
    "1. use the method explain(mode=\"extended\") on the spark.sql statement and look  at the different plans Spark created to excecute the query.\n",
    "2. Read the physical plan from bottom to top and try to match the plan with the query you wrote. (Exchange means that the data is being shuffled between the executors)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias('avg('salary), None)]\n",
      "+- 'UnresolvedRelation [tbl_employees], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "avg(salary): double\n",
      "Aggregate [avg(salary#20) AS avg(salary)#91]\n",
      "+- SubqueryAlias tbl_employees\n",
      "   +- View (`tbl_employees`, [employee_id#17,name#18,department#19,salary#20,hire_date#21])\n",
      "      +- Relation [employee_id#17,name#18,department#19,salary#20,hire_date#21] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [avg(salary#20) AS avg(salary)#91]\n",
      "+- Project [salary#20]\n",
      "   +- Relation [employee_id#17,name#18,department#19,salary#20,hire_date#21] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[avg(salary#20)], output=[avg(salary)#91])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=159]\n",
      "      +- HashAggregate(keys=[], functions=[partial_avg(salary#20)], output=[sum#95, count#96L])\n",
      "         +- FileScan csv [salary#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/DevProjects/Databricks/FileStore/tables/employees.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<salary:int>\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select avg(salary) from tbl_employees\").explain(mode=\"extended\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:39:27.171536400Z",
     "start_time": "2024-09-10T09:39:27.087735400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Go to the SparkUI in the tab SQL/Dataframe\n",
    "1. Search for the query plans in the SparkUI SQL tab.\n",
    "2. Try to understand the excution plan.\n",
    "3. Go to https://dzone.com/articles/debugging-spark-performance-using-explain-plan to get some insights in the operators of the plan. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Find the highest salary in each department in two ways:\n",
    "-  Via the dataframe operation 'groupBy'\n",
    "-  With a sql statement ont tbl_employees"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "| department|highest_salary|\n",
      "+-----------+--------------+\n",
      "|         HR|          4800|\n",
      "|  Marketing|          4300|\n",
      "|Engineering|          6000|\n",
      "+-----------+--------------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['department], ['department, 'max('salary) AS highest_salary#121]\n",
      "+- 'UnresolvedRelation [tbl_employees], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "department: string, highest_salary: int\n",
      "Aggregate [department#19], [department#19, max(salary#20) AS highest_salary#121]\n",
      "+- SubqueryAlias tbl_employees\n",
      "   +- View (`tbl_employees`, [employee_id#17,name#18,department#19,salary#20,hire_date#21])\n",
      "      +- Relation [employee_id#17,name#18,department#19,salary#20,hire_date#21] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [department#19], [department#19, max(salary#20) AS highest_salary#121]\n",
      "+- Project [department#19, salary#20]\n",
      "   +- Relation [employee_id#17,name#18,department#19,salary#20,hire_date#21] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[department#19], functions=[max(salary#20)], output=[department#19, highest_salary#121])\n",
      "   +- Exchange hashpartitioning(department#19, 4), ENSURE_REQUIREMENTS, [plan_id=213]\n",
      "      +- HashAggregate(keys=[department#19], functions=[partial_max(salary#20)], output=[department#19, max#126])\n",
      "         +- FileScan csv [department#19,salary#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/DevProjects/Databricks/FileStore/tables/employees.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<department:string,salary:int>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "highestSalariesByDepartment = df.groupBy(\"department\").agg(max(\"salary\").alias(\"highest_salary\"))\n",
    "highestSalariesByDepartment.show()\n",
    "\n",
    "spark.sql(f'select department, max(salary) as highest_salary from tbl_employees group by department' ).explain(mode=\"extended\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:39:29.492853600Z",
     "start_time": "2024-09-10T09:39:27.171536400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculate the total salary expenditure for each year"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------+\n",
      "|year|total_salary_expenditure|\n",
      "+----+------------------------+\n",
      "|2019|                    8800|\n",
      "|2021|                   10300|\n",
      "|2020|                    9200|\n",
      "|2022|                    8700|\n",
      "|2018|                    6000|\n",
      "|2023|                    5200|\n",
      "+----+------------------------+\n",
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['year('hire_date)], [unresolvedalias('year('hire_date), None), 'sum('salary) AS total_salary_expenditure#153]\n",
      "+- 'UnresolvedRelation [tbl_employees], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "year(hire_date): int, total_salary_expenditure: bigint\n",
      "Aggregate [year(hire_date#21)], [year(hire_date#21) AS year(hire_date)#155, sum(salary#20) AS total_salary_expenditure#153L]\n",
      "+- SubqueryAlias tbl_employees\n",
      "   +- View (`tbl_employees`, [employee_id#17,name#18,department#19,salary#20,hire_date#21])\n",
      "      +- Relation [employee_id#17,name#18,department#19,salary#20,hire_date#21] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [_groupingexpression#158], [_groupingexpression#158 AS year(hire_date)#155, sum(salary#20) AS total_salary_expenditure#153L]\n",
      "+- Project [salary#20, year(hire_date#21) AS _groupingexpression#158]\n",
      "   +- Relation [employee_id#17,name#18,department#19,salary#20,hire_date#21] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[_groupingexpression#158], functions=[sum(salary#20)], output=[year(hire_date)#155, total_salary_expenditure#153L])\n",
      "   +- Exchange hashpartitioning(_groupingexpression#158, 4), ENSURE_REQUIREMENTS, [plan_id=276]\n",
      "      +- HashAggregate(keys=[_groupingexpression#158], functions=[partial_sum(salary#20)], output=[_groupingexpression#158, sum#160L])\n",
      "         +- Project [salary#20, year(hire_date#21) AS _groupingexpression#158]\n",
      "            +- FileScan csv [salary#20,hire_date#21] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/DevProjects/Databricks/FileStore/tables/employees.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<salary:int,hire_date:date>\n"
     ]
    }
   ],
   "source": [
    "totalSalaryExpenditureByYear = df.groupBy(year(\"hire_date\").alias(\"year\")).agg(sum(\"salary\").alias(\"total_salary_expenditure\"))\n",
    "totalSalaryExpenditureByYear.show()\n",
    "\n",
    "spark.sql(f\"select year(hire_date), sum(salary) as total_salary_expenditure from tbl_employees group by year(hire_date)\").explain(mode=\"extended\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:39:31.922418800Z",
     "start_time": "2024-09-10T09:39:28.163419100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculate the number of employees per postal code\n",
    "Postal codes are available in the parquet file empPostalCodes\n",
    "Create a view for the parquet file and join the two datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['postal_code], ['p.postal_code, 'count('e.employee_id) AS number_of_employees#165]\n",
      "+- 'Join Inner, ('e.employee_id = 'p.emp_id)\n",
      "   :- 'SubqueryAlias e\n",
      "   :  +- 'UnresolvedRelation [tbl_employees], [], false\n",
      "   +- 'SubqueryAlias p\n",
      "      +- 'UnresolvedRelation [tbl_empPostalCodes], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "postal_code: string, number_of_employees: bigint\n",
      "Aggregate [postal_code#162], [postal_code#162, count(employee_id#17) AS number_of_employees#165L]\n",
      "+- Join Inner, (employee_id#17 = cast(emp_id#161 as int))\n",
      "   :- SubqueryAlias e\n",
      "   :  +- SubqueryAlias tbl_employees\n",
      "   :     +- View (`tbl_employees`, [employee_id#17,name#18,department#19,salary#20,hire_date#21])\n",
      "   :        +- Relation [employee_id#17,name#18,department#19,salary#20,hire_date#21] csv\n",
      "   +- SubqueryAlias p\n",
      "      +- SubqueryAlias tbl_emppostalcodes\n",
      "         +- View (`tbl_empPostalCodes`, [emp_id#161,postal_code#162])\n",
      "            +- Relation [emp_id#161,postal_code#162] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [postal_code#162], [postal_code#162, count(employee_id#17) AS number_of_employees#165L]\n",
      "+- Project [employee_id#17, postal_code#162]\n",
      "   +- Join Inner, (employee_id#17 = cast(emp_id#161 as int))\n",
      "      :- Project [employee_id#17]\n",
      "      :  +- Filter isnotnull(employee_id#17)\n",
      "      :     +- Relation [employee_id#17,name#18,department#19,salary#20,hire_date#21] csv\n",
      "      +- Filter isnotnull(emp_id#161)\n",
      "         +- Relation [emp_id#161,postal_code#162] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[postal_code#162], functions=[count(employee_id#17)], output=[postal_code#162, number_of_employees#165L])\n",
      "   +- Exchange hashpartitioning(postal_code#162, 4), ENSURE_REQUIREMENTS, [plan_id=321]\n",
      "      +- HashAggregate(keys=[postal_code#162], functions=[partial_count(employee_id#17)], output=[postal_code#162, count#170L])\n",
      "         +- Project [employee_id#17, postal_code#162]\n",
      "            +- BroadcastHashJoin [employee_id#17], [cast(emp_id#161 as int)], Inner, BuildLeft, false\n",
      "               :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=316]\n",
      "               :  +- Filter isnotnull(employee_id#17)\n",
      "               :     +- FileScan csv [employee_id#17] Batched: false, DataFilters: [isnotnull(employee_id#17)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/DevProjects/Databricks/FileStore/tables/employees.csv], PartitionFilters: [], PushedFilters: [IsNotNull(employee_id)], ReadSchema: struct<employee_id:int>\n",
      "               +- Filter isnotnull(emp_id#161)\n",
      "                  +- FileScan parquet [emp_id#161,postal_code#162] Batched: true, DataFilters: [isnotnull(emp_id#161)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/DevProjects/Databricks/FileStore/tables/empPostalCodes], PartitionFilters: [], PushedFilters: [IsNotNull(emp_id)], ReadSchema: struct<emp_id:string,postal_code:string>\n"
     ]
    }
   ],
   "source": [
    "df_PC =spark.read.format(\"parquet\").load(\"./FileStore/tables/empPostalCodes\")\n",
    "df_PC.createOrReplaceTempView(\"tbl_empPostalCodes\")\n",
    "df_empPerPc = spark.sql(\"select p.postal_code, count(e.employee_id) as number_of_employees from tbl_employees e inner join tbl_empPostalCodes p on e.employee_id = p.emp_id group by postal_code\")\n",
    "df_empPerPc.explain(mode=\"extended\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:39:35.136079900Z",
     "start_time": "2024-09-10T09:39:31.763226Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Write the results to a DeltaTable in the spark-warehouse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "df_empPerPc.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"employeesPerPostalCode\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:39:52.915732500Z",
     "start_time": "2024-09-10T09:39:35.068066600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:40:02.008062900Z",
     "start_time": "2024-09-10T09:40:01.188513600Z"
    }
   },
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stuff to create the excercises. Not part of the excercise"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "#df_PC =spark.read.format(\"csv\").option(\"header\", \"true\").load(\"./FileStore/tables/empPostalCodes.csv\")\n",
    "#df_PC.write.format(\"parquet\").mode(\"overwrite\").save(\"./FileStore/tables/empPostalCodes\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:39:52.944995500Z",
     "start_time": "2024-09-10T09:39:52.920174800Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
